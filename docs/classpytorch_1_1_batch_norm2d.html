<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>pytorch-inference: pytorch::BatchNorm2d Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">pytorch-inference
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('classpytorch_1_1_batch_norm2d.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pri-attribs">Private Attributes</a> &#124;
<a href="classpytorch_1_1_batch_norm2d-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">pytorch::BatchNorm2d Class Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p><code>#include &lt;<a class="el" href="_normalization_8hpp_source.html">Normalization.hpp</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for pytorch::BatchNorm2d:</div>
<div class="dyncontent">
 <div class="center">
  <img src="classpytorch_1_1_batch_norm2d.png" usemap="#pytorch::BatchNorm2d_map" alt=""/>
  <map id="pytorch::BatchNorm2d_map" name="pytorch::BatchNorm2d_map">
<area href="classpytorch_1_1_layer.html" alt="pytorch::Layer" shape="rect" coords="0,0,137,24"/>
</map>
 </div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:af6150958cbef2a431c2459a4dd4e19dc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#af6150958cbef2a431c2459a4dd4e19dc">BatchNorm2d</a> (const tensor &amp;<a class="el" href="classpytorch_1_1_batch_norm2d.html#a4ad15d26ec95ee6c5bded61c60d2b9ab">gamma</a>, const tensor &amp;<a class="el" href="classpytorch_1_1_batch_norm2d.html#ad28fb520df906cfc358037e5f462a182">beta</a>, const tensor &amp;<a class="el" href="classpytorch_1_1_batch_norm2d.html#a2deef97aada13c1572aa2f25e0bb4a02">running_mean</a>, const tensor &amp;<a class="el" href="classpytorch_1_1_batch_norm2d.html#a9dbe27b269e4e48b7cb06e316e06846b">running_var</a>, const float &amp;<a class="el" href="classpytorch_1_1_batch_norm2d.html#a91d1c372502d1d18f5fb8ce90866e8c0">epsilon</a>=1e-5)</td></tr>
<tr class="memdesc:af6150958cbef2a431c2459a4dd4e19dc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructs a <a class="el" href="classpytorch_1_1_batch_norm2d.html">BatchNorm2d</a> object.  <a href="#af6150958cbef2a431c2459a4dd4e19dc">More...</a><br /></td></tr>
<tr class="separator:af6150958cbef2a431c2459a4dd4e19dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a910802963973f5d64fb6d281635a1035"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#a910802963973f5d64fb6d281635a1035">BatchNorm2d</a> (const std::string &amp;gamma_filename=&quot;&quot;, const std::vector&lt; int &gt; &amp;gamma_dims={}, const std::string &amp;beta_filename=&quot;&quot;, const std::vector&lt; int &gt; &amp;beta_dims={}, const std::string &amp;running_mean_filename=&quot;&quot;, const std::vector&lt; int &gt; &amp;running_mean_dims={}, const std::string &amp;running_var_filename=&quot;&quot;, const std::vector&lt; int &gt; &amp;running_var_dims={}, const float &amp;<a class="el" href="classpytorch_1_1_batch_norm2d.html#a91d1c372502d1d18f5fb8ce90866e8c0">epsilon</a>=1e-5, const std::string &amp;python_home=&quot;../scripts&quot;)</td></tr>
<tr class="memdesc:a910802963973f5d64fb6d281635a1035"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructs a <a class="el" href="classpytorch_1_1_batch_norm2d.html">BatchNorm2d</a> object and loads the requisite tensors in from filenames and sizes.  <a href="#a910802963973f5d64fb6d281635a1035">More...</a><br /></td></tr>
<tr class="separator:a910802963973f5d64fb6d281635a1035"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af29892810fccc85d2bc202c16c5d49fe"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#af29892810fccc85d2bc202c16c5d49fe">add_gamma</a> (const std::string &amp;gamma_filename=&quot;&quot;, const std::vector&lt; int &gt; &amp;gamma_dims={})</td></tr>
<tr class="memdesc:af29892810fccc85d2bc202c16c5d49fe"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds gamma to the layer if the name wasn't passed to the constructor.  <a href="#af29892810fccc85d2bc202c16c5d49fe">More...</a><br /></td></tr>
<tr class="separator:af29892810fccc85d2bc202c16c5d49fe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a54664c298098621bc625e899c96f7af6"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#a54664c298098621bc625e899c96f7af6">add_beta</a> (const std::string &amp;beta_filename=&quot;&quot;, const std::vector&lt; int &gt; &amp;beta_dims={})</td></tr>
<tr class="memdesc:a54664c298098621bc625e899c96f7af6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds beta if it wasn't added by the constructor.  <a href="#a54664c298098621bc625e899c96f7af6">More...</a><br /></td></tr>
<tr class="separator:a54664c298098621bc625e899c96f7af6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a44601eef2233bbf8cb6da963bcf1520e"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#a44601eef2233bbf8cb6da963bcf1520e">add_running_mean</a> (const std::string &amp;running_mean_filename=&quot;&quot;, const std::vector&lt; int &gt; &amp;running_mean_dims={})</td></tr>
<tr class="memdesc:a44601eef2233bbf8cb6da963bcf1520e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds running_mean if it wasn't added by the constructor.  <a href="#a44601eef2233bbf8cb6da963bcf1520e">More...</a><br /></td></tr>
<tr class="separator:a44601eef2233bbf8cb6da963bcf1520e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1816381a3443fc87ee9b25efd0cfe6e3"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#a1816381a3443fc87ee9b25efd0cfe6e3">add_running_var</a> (const std::string &amp;running_var_filename=&quot;&quot;, const std::vector&lt; int &gt; &amp;running_var_dims={})</td></tr>
<tr class="memdesc:a1816381a3443fc87ee9b25efd0cfe6e3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds running_var if it wasn't added by the constructor.  <a href="#a1816381a3443fc87ee9b25efd0cfe6e3">More...</a><br /></td></tr>
<tr class="separator:a1816381a3443fc87ee9b25efd0cfe6e3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf65015a525817d77ff213de019abbfd"><td class="memItemLeft" align="right" valign="top">std::vector&lt; tensor &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#abf65015a525817d77ff213de019abbfd">forward</a> (const std::vector&lt; tensor &gt; &amp;input)</td></tr>
<tr class="memdesc:abf65015a525817d77ff213de019abbfd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the forward pass of batch normalization.  <a href="#abf65015a525817d77ff213de019abbfd">More...</a><br /></td></tr>
<tr class="separator:abf65015a525817d77ff213de019abbfd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a545dd02286924e89b9446ae24be19193"><td class="memItemLeft" align="right" valign="top">std::vector&lt; tensor &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#a545dd02286924e89b9446ae24be19193">operator()</a> (const std::vector&lt; tensor &gt; &amp;input)</td></tr>
<tr class="memdesc:a545dd02286924e89b9446ae24be19193"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the forward pass of batch normalization.  <a href="#a545dd02286924e89b9446ae24be19193">More...</a><br /></td></tr>
<tr class="separator:a545dd02286924e89b9446ae24be19193"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pri-attribs"></a>
Private Attributes</h2></td></tr>
<tr class="memitem:a4ad15d26ec95ee6c5bded61c60d2b9ab"><td class="memItemLeft" align="right" valign="top">tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#a4ad15d26ec95ee6c5bded61c60d2b9ab">gamma</a></td></tr>
<tr class="separator:a4ad15d26ec95ee6c5bded61c60d2b9ab"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad28fb520df906cfc358037e5f462a182"><td class="memItemLeft" align="right" valign="top">tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#ad28fb520df906cfc358037e5f462a182">beta</a></td></tr>
<tr class="separator:ad28fb520df906cfc358037e5f462a182"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2deef97aada13c1572aa2f25e0bb4a02"><td class="memItemLeft" align="right" valign="top">tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#a2deef97aada13c1572aa2f25e0bb4a02">running_mean</a></td></tr>
<tr class="separator:a2deef97aada13c1572aa2f25e0bb4a02"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9dbe27b269e4e48b7cb06e316e06846b"><td class="memItemLeft" align="right" valign="top">tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#a9dbe27b269e4e48b7cb06e316e06846b">running_var</a></td></tr>
<tr class="separator:a9dbe27b269e4e48b7cb06e316e06846b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a91d1c372502d1d18f5fb8ce90866e8c0"><td class="memItemLeft" align="right" valign="top">float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#a91d1c372502d1d18f5fb8ce90866e8c0">epsilon</a></td></tr>
<tr class="separator:a91d1c372502d1d18f5fb8ce90866e8c0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0f155750ee518f0284d59e31030ad6d9"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classpycpp_1_1py__object.html">pycpp::py_object</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classpytorch_1_1_batch_norm2d.html#a0f155750ee518f0284d59e31030ad6d9">utils</a></td></tr>
<tr class="separator:a0f155750ee518f0284d59e31030ad6d9"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="af6150958cbef2a431c2459a4dd4e19dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af6150958cbef2a431c2459a4dd4e19dc">&#9670;&nbsp;</a></span>BatchNorm2d() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">pytorch::BatchNorm2d::BatchNorm2d </td>
          <td>(</td>
          <td class="paramtype">const tensor &amp;&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const tensor &amp;&#160;</td>
          <td class="paramname"><em>beta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const tensor &amp;&#160;</td>
          <td class="paramname"><em>running_mean</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const tensor &amp;&#160;</td>
          <td class="paramname"><em>running_var</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float &amp;&#160;</td>
          <td class="paramname"><em>epsilon</em> = <code>1e-5</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Constructs a <a class="el" href="classpytorch_1_1_batch_norm2d.html">BatchNorm2d</a> object. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">gamma</td><td>The multiplier for the affine transform. Saved as 'bn.weight' by pytorch. </td></tr>
    <tr><td class="paramname">beta</td><td>The bias for the affine transform. Saved as 'bn.bias' by pytorch. </td></tr>
    <tr><td class="paramname">running_mean</td><td>The running mean for the batchnorm operation. </td></tr>
    <tr><td class="paramname">running_var</td><td>The running variance for the batchnorm operation. </td></tr>
    <tr><td class="paramname">epsilon</td><td>A factor in the denominator of the transform that adds stability. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a910802963973f5d64fb6d281635a1035"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a910802963973f5d64fb6d281635a1035">&#9670;&nbsp;</a></span>BatchNorm2d() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">pytorch::BatchNorm2d::BatchNorm2d </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>gamma_filename</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; int &gt; &amp;&#160;</td>
          <td class="paramname"><em>gamma_dims</em> = <code>{}</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>beta_filename</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; int &gt; &amp;&#160;</td>
          <td class="paramname"><em>beta_dims</em> = <code>{}</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>running_mean_filename</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; int &gt; &amp;&#160;</td>
          <td class="paramname"><em>running_mean_dims</em> = <code>{}</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>running_var_filename</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; int &gt; &amp;&#160;</td>
          <td class="paramname"><em>running_var_dims</em> = <code>{}</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float &amp;&#160;</td>
          <td class="paramname"><em>epsilon</em> = <code>1e-5</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>python_home</em> = <code>&quot;../scripts&quot;</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Constructs a <a class="el" href="classpytorch_1_1_batch_norm2d.html">BatchNorm2d</a> object and loads the requisite tensors in from filenames and sizes. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">gamma_filename</td><td>The file where gamma can be found. Will be loaded with numpy.load(filename). </td></tr>
    <tr><td class="paramname">gamma_dims</td><td>The dimensions of gamma in pytorch convention (n, k, h, w) (usually = (1, k, 1, 1)) </td></tr>
    <tr><td class="paramname">beta_filename</td><td>The file where beta can be found. Will be loaded with numpy.load(filename). </td></tr>
    <tr><td class="paramname">beta_dims</td><td>The dimensions of beta in pytorch convention (n, k, h, w) (usually = (1, k, 1, 1)) </td></tr>
    <tr><td class="paramname">running_mean_filename</td><td>The file where running_mean can be found. Will be loaded with numpy.load(filename). </td></tr>
    <tr><td class="paramname">running_mean_dims</td><td>The dimensions of running_mean in pytorch convention (n, k, h, w) (usually = (1, k, 1, 1)) </td></tr>
    <tr><td class="paramname">running_var_filename</td><td>the file where running_var can be found. Will be loaded with numpy.load(filename). </td></tr>
    <tr><td class="paramname">running_var_dims</td><td>The dimensions of running_var in pytorch convention (n, k, h, w) (usually = (1, k, 1, 1)) </td></tr>
    <tr><td class="paramname">epsilon</td><td>A float for numerical stability, 1e-5 by default. </td></tr>
    <tr><td class="paramname">python_home</td><td>Where the utility scripts are - holds the loading script necessary to load up the tensors. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a54664c298098621bc625e899c96f7af6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a54664c298098621bc625e899c96f7af6">&#9670;&nbsp;</a></span>add_beta()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void pytorch::BatchNorm2d::add_beta </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>beta_filename</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; int &gt; &amp;&#160;</td>
          <td class="paramname"><em>beta_dims</em> = <code>{}</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Adds beta if it wasn't added by the constructor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">beta_filename</td><td>The file where beta can be found. Will be loaded with numpy.load(filename). </td></tr>
    <tr><td class="paramname">beta_dims</td><td>The dimensions of beta in pytorch convention (n, k, h, w) (usually = (1, k, 1, 1)) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="af29892810fccc85d2bc202c16c5d49fe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af29892810fccc85d2bc202c16c5d49fe">&#9670;&nbsp;</a></span>add_gamma()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void pytorch::BatchNorm2d::add_gamma </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>gamma_filename</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; int &gt; &amp;&#160;</td>
          <td class="paramname"><em>gamma_dims</em> = <code>{}</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Adds gamma to the layer if the name wasn't passed to the constructor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">gamma_filename</td><td>The file where gamma can be found. Will be loaded with numpy.load(filename). </td></tr>
    <tr><td class="paramname">gamma_dims</td><td>The dimensions of gamma in pytorch convention (n, k, h, w) (usually = (1, k, 1, 1)) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a44601eef2233bbf8cb6da963bcf1520e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a44601eef2233bbf8cb6da963bcf1520e">&#9670;&nbsp;</a></span>add_running_mean()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void pytorch::BatchNorm2d::add_running_mean </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>running_mean_filename</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; int &gt; &amp;&#160;</td>
          <td class="paramname"><em>running_mean_dims</em> = <code>{}</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Adds running_mean if it wasn't added by the constructor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">running_mean_filename</td><td>The file where running_mean can be found. Will be loaded with numpy.load(filename). </td></tr>
    <tr><td class="paramname">running_mean_dims</td><td>The dimensions of running_mean in pytorch convention (n, k, h, w) (usually = (1, k, 1, 1)) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a1816381a3443fc87ee9b25efd0cfe6e3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1816381a3443fc87ee9b25efd0cfe6e3">&#9670;&nbsp;</a></span>add_running_var()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void pytorch::BatchNorm2d::add_running_var </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>running_var_filename</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; int &gt; &amp;&#160;</td>
          <td class="paramname"><em>running_var_dims</em> = <code>{}</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Adds running_var if it wasn't added by the constructor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">running_var_filename</td><td>the file where running_var can be found. Will be loaded with numpy.load(filename). </td></tr>
    <tr><td class="paramname">running_var_dims</td><td>The dimensions of running_var in pytorch convention (n, k, h, w) (usually = (1, k, 1, 1)) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="abf65015a525817d77ff213de019abbfd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abf65015a525817d77ff213de019abbfd">&#9670;&nbsp;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;tensor&gt; pytorch::BatchNorm2d::forward </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; tensor &gt; &amp;&#160;</td>
          <td class="paramname"><em>input</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Applies the forward pass of batch normalization. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input data to be normalized. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The normalized data. The size has not changed. </dd></dl>

<p>Implements <a class="el" href="classpytorch_1_1_layer.html#aa4eef41dde3e7a2b6853e4ff4d553ed3">pytorch::Layer</a>.</p>

</div>
</div>
<a id="a545dd02286924e89b9446ae24be19193"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a545dd02286924e89b9446ae24be19193">&#9670;&nbsp;</a></span>operator()()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;tensor&gt; pytorch::BatchNorm2d::operator() </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; tensor &gt; &amp;&#160;</td>
          <td class="paramname"><em>input</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Applies the forward pass of batch normalization. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input data to be normalized. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The normalized data. The size has not changed. </dd></dl>

<p>Implements <a class="el" href="classpytorch_1_1_layer.html#a99a644aa88713cb9f6803224c5156f24">pytorch::Layer</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="ad28fb520df906cfc358037e5f462a182"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad28fb520df906cfc358037e5f462a182">&#9670;&nbsp;</a></span>beta</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">tensor pytorch::BatchNorm2d::beta</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a91d1c372502d1d18f5fb8ce90866e8c0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a91d1c372502d1d18f5fb8ce90866e8c0">&#9670;&nbsp;</a></span>epsilon</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">float pytorch::BatchNorm2d::epsilon</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a4ad15d26ec95ee6c5bded61c60d2b9ab"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4ad15d26ec95ee6c5bded61c60d2b9ab">&#9670;&nbsp;</a></span>gamma</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">tensor pytorch::BatchNorm2d::gamma</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a2deef97aada13c1572aa2f25e0bb4a02"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2deef97aada13c1572aa2f25e0bb4a02">&#9670;&nbsp;</a></span>running_mean</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">tensor pytorch::BatchNorm2d::running_mean</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a9dbe27b269e4e48b7cb06e316e06846b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9dbe27b269e4e48b7cb06e316e06846b">&#9670;&nbsp;</a></span>running_var</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">tensor pytorch::BatchNorm2d::running_var</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a0f155750ee518f0284d59e31030ad6d9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0f155750ee518f0284d59e31030ad6d9">&#9670;&nbsp;</a></span>utils</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classpycpp_1_1py__object.html">pycpp::py_object</a> pytorch::BatchNorm2d::utils</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li><a class="el" href="_normalization_8hpp_source.html">Normalization.hpp</a></li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacepytorch.html">pytorch</a></li><li class="navelem"><a class="el" href="classpytorch_1_1_batch_norm2d.html">BatchNorm2d</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
